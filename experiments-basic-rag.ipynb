{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comparing RAG Retrieval Strategies on the Indian Constitution\n\nWe compare five retrieval strategies — No RAG, Naive RAG, Reranker, Hybrid Search (BM25 + semantic), and Hierarchical RAG — on a 404-page PDF of the Indian Constitution, evaluated against a 35-question benchmark using GPT-4o as judge.",
   "id": "7c54c617"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "print(\"OPENAI_API_KEY is set:\", bool(os.environ.get(\"OPENAI_API_KEY\")))\n",
    "print(\"TOGETHER_API_KEY is set:\", bool(os.environ.get(\"TOGETHER_API_KEY\")))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cf21ed9a"
  },
  {
   "cell_type": "markdown",
   "id": "jpjw444zdn",
   "source": "## Setup\n\nLoad API keys from a `.env` file in the parent directory. You'll need:\n- **OPENAI_API_KEY** — for `text-embedding-3-small` embeddings and GPT-4o evaluation\n- **TOGETHER_API_KEY** — for `Llama-3.2-3B-Instruct-Turbo` generation via Together AI",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import fitz  # pymupdf\n",
    "\n",
    "doc = fitz.open(\"indiaconstitution.pdf\")\n",
    "\n",
    "# Extract all text page by page\n",
    "pages = []\n",
    "for page in doc:\n",
    "    text = page.get_text()\n",
    "    pages.append(text)\n",
    "\n",
    "total_chars = sum(len(p) for p in pages)\n",
    "print(f\"Total pages: {len(doc)}\")\n",
    "print(f\"Total characters: {total_chars:,}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "yoxso60uu6j"
  },
  {
   "cell_type": "markdown",
   "id": "7rceaik0uxr",
   "source": "## Load and preview the document",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "for page_num in [0, 5, 50]:\n    if page_num < len(pages):\n        print(f\"{'='*60}\")\n        print(f\"PAGE {page_num + 1}\")\n        print(f\"{'='*60}\")\n        print(pages[page_num][:1000])\n        print(\"...\\n\")\n\nall_text = \"\\n\".join(pages)",
   "execution_count": null,
   "outputs": [],
   "id": "qsucsq9s00n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RAG Pipeline — Base Class\n\nAbstract base class that all pipelines inherit from. Handles embedding (OpenAI), vector storage (ChromaDB), and generation (Together AI). Subclasses override `chunk()`, `retrieve()`, and `generate()`.",
   "id": "6b45db67"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class RAGPipeline(ABC):\n",
    "    \"\"\"Base class for RAG pipelines. Subclass and override methods to swap techniques.\"\"\"\n",
    "\n",
    "    name: str = \"base\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.openai_client = OpenAI()\n",
    "        self.together_client = OpenAI(\n",
    "            api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
    "            base_url=\"https://api.together.xyz/v1\",\n",
    "        )\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        col_name = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", self.name)[:60]\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=col_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"},\n",
    "        )\n",
    "        self.chunks: list[str] = []\n",
    "\n",
    "    # ── Override these in subclasses ──────────────────────────────\n",
    "\n",
    "    @abstractmethod\n",
    "    def chunk(self, text: str) -> list[str]:\n",
    "        \"\"\"Split raw text into chunks.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def embed(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Embed a batch of texts in sub-batches to stay within API limits.\"\"\"\n",
    "        MAX_CHARS = 30_000\n",
    "        SUB_BATCH = 20\n",
    "        truncated = [t[:MAX_CHARS] for t in texts]\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(truncated), SUB_BATCH):\n",
    "            batch = truncated[i : i + SUB_BATCH]\n",
    "            resp = self.openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\", input=batch\n",
    "            )\n",
    "            all_embeddings.extend([item.embedding for item in resp.data])\n",
    "        return all_embeddings\n",
    "\n",
    "    @abstractmethod\n",
    "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
    "        \"\"\"Return context string for a question.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, question: str, context: str) -> str:\n",
    "        \"\"\"Generate an answer given question + retrieved context.\"\"\"\n",
    "        ...\n",
    "\n",
    "    # ── Shared infrastructure (usually not overridden) ───────────\n",
    "\n",
    "    def build_index(self, text: str, batch_size: int = 100):\n",
    "        \"\"\"Chunk text, embed, and upsert into ChromaDB.\"\"\"\n",
    "        self.chunks = self.chunk(text)\n",
    "        print(f\"[{self.name}] {len(self.chunks)} chunks (avg {sum(len(c) for c in self.chunks) // len(self.chunks)} chars)\")\n",
    "\n",
    "        for i in range(0, len(self.chunks), batch_size):\n",
    "            batch = self.chunks[i : i + batch_size]\n",
    "            ids = [f\"chunk_{j}\" for j in range(i, i + len(batch))]\n",
    "            embeddings = self.embed(batch)\n",
    "            self.collection.upsert(ids=ids, documents=batch, embeddings=embeddings)\n",
    "            print(f\"  Embedded batch {i // batch_size + 1} ({len(batch)} chunks)\")\n",
    "\n",
    "        print(f\"  Total in collection: {self.collection.count()}\")\n",
    "\n",
    "    def run(self, question: str) -> tuple[str, str]:\n",
    "        \"\"\"End-to-end: retrieve context then generate answer.\"\"\"\n",
    "        context = self.retrieve(question)\n",
    "        answer = self.generate(question, context)\n",
    "        return answer, context\n",
    "\n",
    "    def generate_answers(self, val_path: str = \"val.json\") -> list[dict]:\n",
    "        \"\"\"Run the pipeline on a validation set and return results.\"\"\"\n",
    "        with open(val_path) as f:\n",
    "            val_data = json.load(f)\n",
    "\n",
    "        print(f\"[{self.name}] Running on {len(val_data)} questions\")\n",
    "\n",
    "        results = []\n",
    "        for item in val_data:\n",
    "            q = item[\"question\"]\n",
    "            print(f\"\\n[{item['id']}] {item['category']}\")\n",
    "            print(f\"Q: {q}\")\n",
    "            answer, context = self.run(q)\n",
    "            results.append({\n",
    "                \"id\": item[\"id\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"question\": q,\n",
    "                \"answer\": item[\"answer\"],\n",
    "                \"citation\": item[\"citation\"],\n",
    "                \"ai_response\": answer,\n",
    "                \"context\": context,\n",
    "            })\n",
    "            print(f\"A: {answer[:200]}...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        return results"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "6bc283df"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class NoRAG(RAGPipeline):\n",
    "    \"\"\"Baseline: no retrieval at all — the LLM answers from its own knowledge.\"\"\"\n",
    "\n",
    "    name = \"no_rag\"\n",
    "\n",
    "    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "\n",
    "    def chunk(self, text: str) -> list[str]:\n",
    "        # No chunking needed\n",
    "        return []\n",
    "\n",
    "    def build_index(self, text: str, batch_size: int = 100):\n",
    "        # No index to build\n",
    "        print(f\"[{self.name}] Skipping index build (no retrieval)\")\n",
    "\n",
    "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
    "        # No retrieval — return empty context\n",
    "        return \"\"\n",
    "\n",
    "    def generate(self, question: str, context: str) -> str:\n",
    "        system = (\n",
    "            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n",
    "            \"Be precise and cite relevant articles when possible.\"\n",
    "        )\n",
    "        resp = self.together_client.chat.completions.create(\n",
    "            model=self.GEN_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nAnswer:\"},\n",
    "            ],\n",
    "        )\n",
    "        return resp.choices[0].message.content"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "e76d53d3"
  },
  {
   "cell_type": "markdown",
   "id": "4zj50npdqvc",
   "source": "### No RAG (Baseline)\n\nNo retrieval — the LLM answers purely from its parametric knowledge.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class NaiveRAG(RAGPipeline):\n",
    "    \"\"\"Fixed-size chunking, cosine similarity retrieval, no reranking.\"\"\"\n",
    "\n",
    "    name = \"naive_rag\"\n",
    "\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "\n",
    "    def chunk(self, text: str) -> list[str]:\n",
    "        chunks, start = [], 0\n",
    "        while start < len(text):\n",
    "            chunks.append(text[start : start + self.CHUNK_SIZE])\n",
    "            start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n",
    "        return chunks\n",
    "\n",
    "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
    "        q_emb = self.embed([question])[0]\n",
    "        results = self.collection.query(query_embeddings=[q_emb], n_results=n_results)\n",
    "        return \"\\n\\n---\\n\\n\".join(results[\"documents\"][0])\n",
    "\n",
    "    def generate(self, question: str, context: str) -> str:\n",
    "        system = (\n",
    "            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n",
    "            \"Use ONLY the provided context to answer. If the context doesn't contain enough \"\n",
    "            \"information, say so. Be precise and cite relevant articles when possible.\"\n",
    "        )\n",
    "        resp = self.together_client.chat.completions.create(\n",
    "            model=self.GEN_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"},\n",
    "            ],\n",
    "        )\n",
    "        return resp.choices[0].message.content"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "ed149813"
  },
  {
   "cell_type": "markdown",
   "id": "azuu9fhh1gl",
   "source": "### Naive RAG\n\nFixed-size chunking (1000 chars, 200 overlap) with cosine similarity retrieval. No reranking.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class RerankerRAG(RAGPipeline):\n",
    "    \"\"\"Reranks initial vector-search candidates using an LLM before generation.\"\"\"\n",
    "\n",
    "    name = \"reranker_rag\"\n",
    "\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "    RERANK_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "    INITIAL_K = 10   # candidates fetched from vector DB\n",
    "    FINAL_K = 5      # candidates kept after reranking\n",
    "\n",
    "    def chunk(self, text: str) -> list[str]:\n",
    "        chunks, start = [], 0\n",
    "        while start < len(text):\n",
    "            chunks.append(text[start : start + self.CHUNK_SIZE])\n",
    "            start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n",
    "        return chunks\n",
    "\n",
    "    def _rerank_with_llm(self, query: str, documents: list[str], top_n: int) -> list[str]:\n",
    "        \"\"\"Ask an LLM to score each document's relevance, return top_n.\"\"\"\n",
    "        scored = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            prompt = (\n",
    "                f\"On a scale of 1-10, rate how relevant the following document is to the query.\\n\"\n",
    "                f\"Query: {query}\\n\\n\"\n",
    "                f\"Document:\\n{doc[:800]}\\n\\n\"\n",
    "                f\"Respond with ONLY a JSON object: {{\\\"score\\\": <integer 1-10>}}\"\n",
    "            )\n",
    "            resp = self.together_client.chat.completions.create(\n",
    "                model=self.RERANK_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "            )\n",
    "            try:\n",
    "                score = int(json.loads(resp.choices[0].message.content)[\"score\"])\n",
    "            except (json.JSONDecodeError, KeyError, ValueError):\n",
    "                score = 1\n",
    "            scored.append((score, i, doc))\n",
    "\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [doc for _, _, doc in scored[:top_n]]\n",
    "\n",
    "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
    "        # Step 1: broad vector search\n",
    "        q_emb = self.embed([question])[0]\n",
    "        results = self.collection.query(query_embeddings=[q_emb], n_results=self.INITIAL_K)\n",
    "        candidates = results[\"documents\"][0]\n",
    "\n",
    "        # Step 2: LLM rerank and keep top FINAL_K\n",
    "        reranked = self._rerank_with_llm(question, candidates, self.FINAL_K)\n",
    "        return \"\\n\\n---\\n\\n\".join(reranked)\n",
    "\n",
    "    def generate(self, question: str, context: str) -> str:\n",
    "        system = (\n",
    "            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n",
    "            \"Use ONLY the provided context to answer. If the context doesn't contain enough \"\n",
    "            \"information, say so. Be precise and cite relevant articles when possible.\"\n",
    "        )\n",
    "        resp = self.together_client.chat.completions.create(\n",
    "            model=self.GEN_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"},\n",
    "            ],\n",
    "        )\n",
    "        return resp.choices[0].message.content"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "pe120xzjsts"
  },
  {
   "cell_type": "markdown",
   "id": "1zjn65m02nz",
   "source": "### Reranker RAG\n\nFetches 10 candidates via vector search, then uses an LLM to score each chunk's relevance (1-10) and keeps the top 5.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from collections import defaultdict\nfrom rank_bm25 import BM25Okapi\nimport re as _re\n\n\nclass HybridSearchRAG(RAGPipeline):\n    \"\"\"Combines BM25 keyword search with semantic vector search using RRF fusion.\"\"\"\n\n    name = \"hybrid_search_rag\"\n\n    CHUNK_SIZE = 1000\n    CHUNK_OVERLAP = 200\n    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n    SEMANTIC_K = 10\n    BM25_K = 10\n    FINAL_K = 5\n    RRF_K = 60  # constant for Reciprocal Rank Fusion\n\n    def chunk(self, text: str) -> list[str]:\n        chunks, start = [], 0\n        while start < len(text):\n            chunks.append(text[start : start + self.CHUNK_SIZE])\n            start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n        return chunks\n\n    def build_index(self, text: str, batch_size: int = 100):\n        \"\"\"Build both vector index and BM25 index.\"\"\"\n        super().build_index(text, batch_size)\n\n        # Build BM25 index over the same chunks\n        tokenized = [self._tokenize(c) for c in self.chunks]\n        self.bm25 = BM25Okapi(tokenized)\n        print(f\"  BM25 index built over {len(self.chunks)} chunks\")\n\n    @staticmethod\n    def _tokenize(text: str) -> list[str]:\n        return _re.findall(r\"\\w+\", text.lower())\n\n    def _bm25_search(self, query: str, k: int) -> list[tuple[int, float]]:\n        \"\"\"Return top-k (chunk_index, score) pairs from BM25.\"\"\"\n        tokens = self._tokenize(query)\n        scores = self.bm25.get_scores(tokens)\n        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n        return [(i, scores[i]) for i in top_indices]\n\n    def _semantic_search(self, query: str, k: int) -> list[tuple[int, float]]:\n        \"\"\"Return top-k (chunk_index, rank_position) pairs from vector search.\"\"\"\n        q_emb = self.embed([query])[0]\n        results = self.collection.query(query_embeddings=[q_emb], n_results=k, include=[\"documents\"])\n        # ChromaDB returns ids like \"chunk_42\" — extract indices\n        indices = [int(cid.split(\"_\")[1]) for cid in results[\"ids\"][0]]\n        return [(idx, rank) for rank, idx in enumerate(indices)]\n\n    def _rrf_fuse(self, semantic_results: list[tuple[int, float]], bm25_results: list[tuple[int, float]]) -> list[int]:\n        \"\"\"Reciprocal Rank Fusion to merge two ranked lists.\"\"\"\n        scores = defaultdict(float)\n\n        for rank, (idx, _) in enumerate(semantic_results):\n            scores[idx] += 1.0 / (self.RRF_K + rank + 1)\n\n        for rank, (idx, _) in enumerate(bm25_results):\n            scores[idx] += 1.0 / (self.RRF_K + rank + 1)\n\n        fused = sorted(scores.keys(), key=lambda i: scores[i], reverse=True)\n        return fused[: self.FINAL_K]\n\n    def retrieve(self, question: str, n_results: int = 5) -> str:\n        semantic = self._semantic_search(question, self.SEMANTIC_K)\n        bm25 = self._bm25_search(question, self.BM25_K)\n        top_indices = self._rrf_fuse(semantic, bm25)\n        return \"\\n\\n---\\n\\n\".join(self.chunks[i] for i in top_indices)\n\n    def generate(self, question: str, context: str) -> str:\n        system = (\n            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n            \"Use ONLY the provided context to answer. If the context doesn't contain enough \"\n            \"information, say so. Be precise and cite relevant articles when possible.\"\n        )\n        resp = self.together_client.chat.completions.create(\n            model=self.GEN_MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": system},\n                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"},\n            ],\n        )\n        return resp.choices[0].message.content",
   "execution_count": null,
   "outputs": [],
   "id": "2om8slw5xbx"
  },
  {
   "cell_type": "markdown",
   "id": "2euunl7vp9c",
   "source": "### Hybrid Search RAG\n\nCombines BM25 keyword search with semantic vector search, merged via Reciprocal Rank Fusion (RRF).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class HierarchicalRAG(RAGPipeline):\n    \"\"\"Two-level retrieval: search summaries first, then detailed chunks within matched sections.\"\"\"\n\n    name = \"hierarchical_rag\"\n\n    SECTION_SIZE = 5000       # large sections for summarisation\n    CHUNK_SIZE = 1000         # detailed chunks within each section\n    CHUNK_OVERLAP = 200\n    SUMMARY_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n    K_SUMMARIES = 3           # top sections to retrieve\n    K_CHUNKS = 5              # detailed chunks per selected section\n\n    def chunk(self, text: str) -> list[str]:\n        \"\"\"Not used directly — build_index handles both levels.\"\"\"\n        chunks, start = [], 0\n        while start < len(text):\n            chunks.append(text[start : start + self.CHUNK_SIZE])\n            start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n        return chunks\n\n    def _make_sections(self, text: str) -> list[str]:\n        \"\"\"Split text into large non-overlapping sections.\"\"\"\n        sections = []\n        start = 0\n        while start < len(text):\n            sections.append(text[start : start + self.SECTION_SIZE])\n            start += self.SECTION_SIZE\n        return sections\n\n    def _summarise(self, section_text: str) -> str:\n        \"\"\"Ask the LLM for a concise summary of a section.\"\"\"\n        resp = self.together_client.chat.completions.create(\n            model=self.SUMMARY_MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": \"Summarise the following legal text in 2-3 sentences. Focus on the key articles, rights, or provisions mentioned.\"},\n                {\"role\": \"user\", \"content\": section_text[:4000]},\n            ],\n            temperature=0,\n        )\n        return resp.choices[0].message.content\n\n    def build_index(self, text: str, batch_size: int = 100):\n        \"\"\"Build two-level index: summary collection + detailed chunk collection.\"\"\"\n        sections = self._make_sections(text)\n        print(f\"[{self.name}] {len(sections)} sections of ~{self.SECTION_SIZE} chars\")\n\n        # ── Level 1: Summaries ──\n        self.summary_collection = self.chroma_client.get_or_create_collection(\n            name=\"hierarchical_summaries\", metadata={\"hnsw:space\": \"cosine\"},\n        )\n        print(\"  Generating summaries...\")\n        self.section_texts = sections\n        summaries = []\n        for i, sec in enumerate(sections):\n            summary = self._summarise(sec)\n            summaries.append(summary)\n            if (i + 1) % 20 == 0:\n                print(f\"    Summarised {i + 1}/{len(sections)} sections\")\n\n        # Embed and store summaries\n        for i in range(0, len(summaries), batch_size):\n            batch = summaries[i : i + batch_size]\n            ids = [f\"summary_{j}\" for j in range(i, i + len(batch))]\n            embeddings = self.embed(batch)\n            self.summary_collection.upsert(ids=ids, documents=batch, embeddings=embeddings)\n        print(f\"  {len(summaries)} summaries indexed\")\n\n        # ── Level 2: Detailed chunks (with section_id metadata) ──\n        self.detailed_collection = self.chroma_client.get_or_create_collection(\n            name=\"hierarchical_details\", metadata={\"hnsw:space\": \"cosine\"},\n        )\n        all_chunks, all_ids, all_meta = [], [], []\n        chunk_counter = 0\n        for sec_idx, section in enumerate(sections):\n            start = 0\n            while start < len(section):\n                chunk_text = section[start : start + self.CHUNK_SIZE]\n                all_chunks.append(chunk_text)\n                all_ids.append(f\"detail_{chunk_counter}\")\n                all_meta.append({\"section_id\": sec_idx})\n                chunk_counter += 1\n                start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n\n        self.chunks = all_chunks  # keep for compatibility\n\n        for i in range(0, len(all_chunks), batch_size):\n            batch_docs = all_chunks[i : i + batch_size]\n            batch_ids = all_ids[i : i + batch_size]\n            batch_meta = all_meta[i : i + batch_size]\n            embeddings = self.embed(batch_docs)\n            self.detailed_collection.upsert(\n                ids=batch_ids, documents=batch_docs,\n                embeddings=embeddings, metadatas=batch_meta,\n            )\n            print(f\"  Embedded detail batch {i // batch_size + 1} ({len(batch_docs)} chunks)\")\n\n        print(f\"  {len(all_chunks)} detailed chunks indexed across {len(sections)} sections\")\n\n    def retrieve(self, question: str, n_results: int = 5) -> str:\n        \"\"\"Two-stage retrieval: summaries → detailed chunks within top sections.\"\"\"\n        q_emb = self.embed([question])[0]\n\n        # Stage 1: find top-K summary sections\n        summary_results = self.summary_collection.query(\n            query_embeddings=[q_emb], n_results=self.K_SUMMARIES,\n        )\n        top_section_ids = [\n            int(sid.split(\"_\")[1]) for sid in summary_results[\"ids\"][0]\n        ]\n\n        # Stage 2: search detailed chunks filtered to those sections\n        detail_results = self.detailed_collection.query(\n            query_embeddings=[q_emb],\n            n_results=self.K_CHUNKS,\n            where={\"section_id\": {\"$in\": top_section_ids}},\n            include=[\"documents\"],\n        )\n\n        return \"\\n\\n---\\n\\n\".join(detail_results[\"documents\"][0])\n\n    def generate(self, question: str, context: str) -> str:\n        system = (\n            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n            \"Use ONLY the provided context to answer. If the context doesn't contain enough \"\n            \"information, say so. Be precise and cite relevant articles when possible.\"\n        )\n        resp = self.together_client.chat.completions.create(\n            model=self.GEN_MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": system},\n                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"},\n            ],\n        )\n        return resp.choices[0].message.content",
   "execution_count": null,
   "outputs": [],
   "id": "z8pp72ckrif"
  },
  {
   "cell_type": "markdown",
   "id": "x4z7jt83qed",
   "source": "### Hierarchical RAG\n\nTwo-level retrieval: first searches LLM-generated section summaries, then retrieves detailed chunks within the matched sections.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Evaluator\n\nLLM-as-a-Judge using GPT-4o. Scores each response against ground truth as PASS (1), PARTIAL (0.5), or FAIL (0), with per-category breakdown.",
   "id": "x3vvibuj6zo"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"LLM-as-a-Judge scorer. Accepts RAG results and scores them against ground truth.\"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. \"\n",
    "        \"If the AI assistant's response is very close to the true response, assign a score of 1. \"\n",
    "        \"If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. \"\n",
    "        \"If the response is partially aligned with the true response, assign a score of 0.5. \"\n",
    "        'Respond with ONLY a JSON object: {\"score\": <0 | 0.5 | 1>}'\n",
    "    )\n",
    "\n",
    "    def __init__(self, judge_model: str = \"gpt-4o\"):\n",
    "        self.judge_model = judge_model\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def _score_one(self, question: str, ai_response: str, true_response: str) -> float:\n",
    "        prompt = (\n",
    "            f\"User Query: {question}\\n\"\n",
    "            f\"AI Response: {ai_response}\\n\"\n",
    "            f\"True Response: {true_response}\\n\"\n",
    "            f\"{self.SYSTEM_PROMPT}\"\n",
    "        )\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.judge_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return float(json.loads(resp.choices[0].message.content)[\"score\"])\n",
    "\n",
    "    def evaluate(self, results: list[dict], generation_time: float | None = None) -> list[dict]:\n",
    "        \"\"\"Score a list of RAG results and print aggregate metrics with timing.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  SCORING — Judge: {self.judge_model}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        eval_start = time.time()\n",
    "\n",
    "        for r in results:\n",
    "            r[\"score\"] = self._score_one(r[\"question\"], r[\"ai_response\"], r[\"answer\"])\n",
    "            label = {1: \"PASS\", 0.5: \"PARTIAL\", 0: \"FAIL\"}[r[\"score\"]]\n",
    "            print(f\"[{r['id']:>2}] {label:>7} ({r['score']})  {r['question'][:80]}\")\n",
    "\n",
    "        eval_time = time.time() - eval_start\n",
    "\n",
    "        # ── Aggregate ──\n",
    "        n = len(results)\n",
    "        total = sum(r[\"score\"] for r in results)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  AGGREGATE SCORE — {n} questions\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Total: {total} / {n}  ({total / n:.1%})\")\n",
    "        print(f\"  PASS: {sum(1 for r in results if r['score'] == 1)}\")\n",
    "        print(f\"  PARTIAL: {sum(1 for r in results if r['score'] == 0.5)}\")\n",
    "        print(f\"  FAIL: {sum(1 for r in results if r['score'] == 0)}\")\n",
    "\n",
    "        # ── Timing ──\n",
    "        print(f\"\\n  TIMING\")\n",
    "        print(f\"  {'─'*50}\")\n",
    "        if generation_time is not None:\n",
    "            print(f\"  Generation:  {generation_time:.1f}s ({generation_time / n:.2f}s per question)\")\n",
    "        print(f\"  Evaluation:  {eval_time:.1f}s ({eval_time / n:.2f}s per question)\")\n",
    "        if generation_time is not None:\n",
    "            total_time = generation_time + eval_time\n",
    "            print(f\"  Total:       {total_time:.1f}s\")\n",
    "\n",
    "        # ── Per-category breakdown ──\n",
    "        categories = sorted(set(r[\"category\"] for r in results))\n",
    "        print(f\"\\n  PER-CATEGORY BREAKDOWN\")\n",
    "        print(f\"  {'─'*50}\")\n",
    "        for cat in categories:\n",
    "            cat_items = [r for r in results if r[\"category\"] == cat]\n",
    "            cat_total = sum(r[\"score\"] for r in cat_items)\n",
    "            cat_n = len(cat_items)\n",
    "            print(f\"  {cat} — {cat_total}/{cat_n} ({cat_total / cat_n:.1%})\")\n",
    "\n",
    "        return results"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "ihkmpek0r2q"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pipeline = RerankerRAG()\n",
    "pipeline.build_index(all_text)\n",
    "\n",
    "gen_start = time.time()\n",
    "results = pipeline.generate_answers()\n",
    "gen_time = time.time() - gen_start\n",
    "\n",
    "evaluator = Evaluator(judge_model=\"gpt-4o\")\n",
    "eval_results = evaluator.evaluate(results, generation_time=gen_time)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d15c0f1b"
  },
  {
   "cell_type": "markdown",
   "id": "kwutq8ywsj",
   "source": "## Run\n\nPick a pipeline, build the index, generate answers, and evaluate. Change the pipeline class below to compare strategies.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}