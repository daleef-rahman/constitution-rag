{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read the constitution pdf"
      ],
      "id": "7c54c617"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path=\"../.env\")\n",
        "\n",
        "print(\"OPENAI_API_KEY is set:\", bool(os.environ.get(\"OPENAI_API_KEY\")))\n",
        "print(\"TOGETHER_API_KEY is set:\", bool(os.environ.get(\"TOGETHER_API_KEY\")))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cf21ed9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import fitz  # pymupdf\n",
        "\n",
        "doc = fitz.open(\"indiaconstitution.pdf\")\n",
        "\n",
        "# Extract all text page by page\n",
        "pages = []\n",
        "for page in doc:\n",
        "    text = page.get_text()\n",
        "    pages.append(text)\n",
        "\n",
        "total_chars = sum(len(p) for p in pages)\n",
        "print(f\"Total pages: {len(doc)}\")\n",
        "print(f\"Total characters: {total_chars:,}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total pages: 404\n",
            "Total characters: 877,147\n"
          ]
        }
      ],
      "id": "yoxso60uu6j"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preview text from a few pages to understand the structure\n",
        "for page_num in [0, 5, 50]:\n",
        "    if page_num < len(pages):\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"PAGE {page_num + 1}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(pages[page_num][:1000])\n",
        "        print(\"...\\n\")\n",
        "\n",
        "# One-click: build index, generate answers, evaluate\n",
        "all_text = \"\\n\".join(pages)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PAGE 1\n",
            "============================================================\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " THE CONSTITUTION OF INDIA \n",
            "[As on       May, 2022] \n",
            "2022 \n",
            " \n",
            "\n",
            "...\n",
            "\n",
            "============================================================\n",
            "PAGE 6\n",
            "============================================================\n",
            "Contents \n",
            " \n",
            "   ARTICLES \n",
            "(iii)\n",
            " \n",
            "Cultural and Educational Rights \n",
            "29. \n",
            "Protection of interests of minorities. \n",
            "30. \n",
            "Right of minorities to establish and administer educational \n",
            "institutions. \n",
            "[31. \n",
            "Compulsory acquisition of property. —Omitted.] \n",
            " \n",
            "Saving of Certain Laws \n",
            "31A. \n",
            "Saving of Laws providing for acquisition of estates, etc. \n",
            "31B. \n",
            "Validation of certain Acts and Regulations. \n",
            "31C. \n",
            "Saving of laws giving effect to certain directive principles. \n",
            "[31D. \n",
            "Saving of laws in respect of anti-national activities.—Omitted.] \n",
            " \n",
            "Right to Constitutional Remedies \n",
            " 32. \n",
            "Remedies for enforcement of rights conferred by this Part. \n",
            "[32A. \n",
            "Constitutional validity of State laws not to be considered in \n",
            "proceedings under article 32.—Omitted.]  \n",
            " 33. \n",
            "Power of Parliament to modify the rights conferred by this Part \n",
            "in their application to Forces, etc. \n",
            " 34. \n",
            "Restriction on rights conferred by this Part while martial law is \n",
            "in force in any area. \n",
            " 35. \n",
            "Legislation to give effect to the provisions \n",
            "...\n",
            "\n",
            "============================================================\n",
            "PAGE 51\n",
            "============================================================\n",
            "THE CONSTITUTION OF  INDIA \n",
            "(Part III.—Fundamental Rights) \n",
            "20\n",
            "34. Restriction on rights conferred by this Part while martial law is \n",
            "in force in any area.—Notwithstanding anything in the foregoing provisions \n",
            "of this Part, Parliament may by law indemnify any person in the service of the \n",
            "Union or of a State or any other person in respect of any act done by him in \n",
            "connection with the maintenance or restoration of order in any area within the \n",
            "territory of India where martial law was in force or validate any sentence \n",
            "passed, punishment inflicted, forfeiture ordered or other act done under martial \n",
            "law in such area. \n",
            "35. Legislation to give effect to the provisions of this Part.—\n",
            "Notwithstanding anything in this Constitution,—  \n",
            "(a) Parliament shall have, and the Legislature of a State shall not \n",
            "have, power to make laws— \n",
            "(i) with respect to any of the matters which under clause (3) of \n",
            "article 16, clause (3) of article 32, article 33 and article 34 may be \n",
            "provided for by law made by\n",
            "...\n",
            "\n"
          ]
        }
      ],
      "id": "qsucsq9s00n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Pipeline"
      ],
      "id": "6b45db67"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import chromadb\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "class RAGPipeline(ABC):\n",
        "    \"\"\"Base class for RAG pipelines. Subclass and override methods to swap techniques.\"\"\"\n",
        "\n",
        "    name: str = \"base\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.openai_client = OpenAI()\n",
        "        self.together_client = OpenAI(\n",
        "            api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "            base_url=\"https://api.together.xyz/v1\",\n",
        "        )\n",
        "        self.chroma_client = chromadb.Client()\n",
        "        col_name = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", self.name)[:60]\n",
        "        self.collection = self.chroma_client.get_or_create_collection(\n",
        "            name=col_name,\n",
        "            metadata={\"hnsw:space\": \"cosine\"},\n",
        "        )\n",
        "        self.chunks: list[str] = []\n",
        "\n",
        "    # ── Override these in subclasses ──────────────────────────────\n",
        "\n",
        "    @abstractmethod\n",
        "    def chunk(self, text: str) -> list[str]:\n",
        "        \"\"\"Split raw text into chunks.\"\"\"\n",
        "        ...\n",
        "\n",
        "    def embed(self, texts: list[str]) -> list[list[float]]:\n",
        "        \"\"\"Embed a batch of texts in sub-batches to stay within API limits.\"\"\"\n",
        "        MAX_CHARS = 30_000\n",
        "        SUB_BATCH = 20\n",
        "        truncated = [t[:MAX_CHARS] for t in texts]\n",
        "        all_embeddings = []\n",
        "        for i in range(0, len(truncated), SUB_BATCH):\n",
        "            batch = truncated[i : i + SUB_BATCH]\n",
        "            resp = self.openai_client.embeddings.create(\n",
        "                model=\"text-embedding-3-small\", input=batch\n",
        "            )\n",
        "            all_embeddings.extend([item.embedding for item in resp.data])\n",
        "        return all_embeddings\n",
        "\n",
        "    @abstractmethod\n",
        "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
        "        \"\"\"Return context string for a question.\"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, question: str, context: str) -> str:\n",
        "        \"\"\"Generate an answer given question + retrieved context.\"\"\"\n",
        "        ...\n",
        "\n",
        "    # ── Shared infrastructure (usually not overridden) ───────────\n",
        "\n",
        "    def build_index(self, text: str, batch_size: int = 100):\n",
        "        \"\"\"Chunk text, embed, and upsert into ChromaDB.\"\"\"\n",
        "        self.chunks = self.chunk(text)\n",
        "        print(f\"[{self.name}] {len(self.chunks)} chunks (avg {sum(len(c) for c in self.chunks) // len(self.chunks)} chars)\")\n",
        "\n",
        "        for i in range(0, len(self.chunks), batch_size):\n",
        "            batch = self.chunks[i : i + batch_size]\n",
        "            ids = [f\"chunk_{j}\" for j in range(i, i + len(batch))]\n",
        "            embeddings = self.embed(batch)\n",
        "            self.collection.upsert(ids=ids, documents=batch, embeddings=embeddings)\n",
        "            print(f\"  Embedded batch {i // batch_size + 1} ({len(batch)} chunks)\")\n",
        "\n",
        "        print(f\"  Total in collection: {self.collection.count()}\")\n",
        "\n",
        "    def run(self, question: str) -> tuple[str, str]:\n",
        "        \"\"\"End-to-end: retrieve context then generate answer.\"\"\"\n",
        "        context = self.retrieve(question)\n",
        "        answer = self.generate(question, context)\n",
        "        return answer, context\n",
        "\n",
        "    def generate_answers(self, val_path: str = \"val.json\") -> list[dict]:\n",
        "        \"\"\"Run the pipeline on a validation set and return results.\"\"\"\n",
        "        with open(val_path) as f:\n",
        "            val_data = json.load(f)\n",
        "\n",
        "        print(f\"[{self.name}] Running on {len(val_data)} questions\")\n",
        "\n",
        "        results = []\n",
        "        for item in val_data:\n",
        "            q = item[\"question\"]\n",
        "            print(f\"\\n[{item['id']}] {item['category']}\")\n",
        "            print(f\"Q: {q}\")\n",
        "            answer, context = self.run(q)\n",
        "            results.append({\n",
        "                \"id\": item[\"id\"],\n",
        "                \"category\": item[\"category\"],\n",
        "                \"question\": q,\n",
        "                \"answer\": item[\"answer\"],\n",
        "                \"citation\": item[\"citation\"],\n",
        "                \"ai_response\": answer,\n",
        "                \"context\": context,\n",
        "            })\n",
        "            print(f\"A: {answer[:200]}...\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        return results"
      ],
      "execution_count": 25,
      "outputs": [],
      "id": "6bc283df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NoRAG(RAGPipeline):\n",
        "    \"\"\"Baseline: no retrieval at all — the LLM answers from its own knowledge.\"\"\"\n",
        "\n",
        "    name = \"no_rag\"\n",
        "\n",
        "    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
        "\n",
        "    def chunk(self, text: str) -> list[str]:\n",
        "        # No chunking needed\n",
        "        return []\n",
        "\n",
        "    def build_index(self, text: str, batch_size: int = 100):\n",
        "        # No index to build\n",
        "        print(f\"[{self.name}] Skipping index build (no retrieval)\")\n",
        "\n",
        "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
        "        # No retrieval — return empty context\n",
        "        return \"\"\n",
        "\n",
        "    def generate(self, question: str, context: str) -> str:\n",
        "        system = (\n",
        "            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n",
        "            \"Be precise and cite relevant articles when possible.\"\n",
        "        )\n",
        "        resp = self.together_client.chat.completions.create(\n",
        "            model=self.GEN_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nAnswer:\"},\n",
        "            ],\n",
        "        )\n",
        "        return resp.choices[0].message.content"
      ],
      "execution_count": 73,
      "outputs": [],
      "id": "e76d53d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NaiveRAG(RAGPipeline):\n",
        "    \"\"\"Fixed-size chunking, cosine similarity retrieval, no reranking.\"\"\"\n",
        "\n",
        "    name = \"naive_rag\"\n",
        "\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
        "\n",
        "    def chunk(self, text: str) -> list[str]:\n",
        "        chunks, start = [], 0\n",
        "        while start < len(text):\n",
        "            chunks.append(text[start : start + self.CHUNK_SIZE])\n",
        "            start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n",
        "        return chunks\n",
        "\n",
        "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
        "        q_emb = self.embed([question])[0]\n",
        "        results = self.collection.query(query_embeddings=[q_emb], n_results=n_results)\n",
        "        return \"\\n\\n---\\n\\n\".join(results[\"documents\"][0])\n",
        "\n",
        "    def generate(self, question: str, context: str) -> str:\n",
        "        system = (\n",
        "            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n",
        "            \"Use ONLY the provided context to answer. If the context doesn't contain enough \"\n",
        "            \"information, say so. Be precise and cite relevant articles when possible.\"\n",
        "        )\n",
        "        resp = self.together_client.chat.completions.create(\n",
        "            model=self.GEN_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"},\n",
        "            ],\n",
        "        )\n",
        "        return resp.choices[0].message.content"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ed149813"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class RerankerRAG(RAGPipeline):\n",
        "    \"\"\"Reranks initial vector-search candidates using an LLM before generation.\"\"\"\n",
        "\n",
        "    name = \"reranker_rag\"\n",
        "\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
        "    RERANK_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
        "    INITIAL_K = 10   # candidates fetched from vector DB\n",
        "    FINAL_K = 5      # candidates kept after reranking\n",
        "\n",
        "    def chunk(self, text: str) -> list[str]:\n",
        "        chunks, start = [], 0\n",
        "        while start < len(text):\n",
        "            chunks.append(text[start : start + self.CHUNK_SIZE])\n",
        "            start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n",
        "        return chunks\n",
        "\n",
        "    def _rerank_with_llm(self, query: str, documents: list[str], top_n: int) -> list[str]:\n",
        "        \"\"\"Ask an LLM to score each document's relevance, return top_n.\"\"\"\n",
        "        scored = []\n",
        "        for i, doc in enumerate(documents):\n",
        "            prompt = (\n",
        "                f\"On a scale of 1-10, rate how relevant the following document is to the query.\\n\"\n",
        "                f\"Query: {query}\\n\\n\"\n",
        "                f\"Document:\\n{doc[:800]}\\n\\n\"\n",
        "                f\"Respond with ONLY a JSON object: {{\\\"score\\\": <integer 1-10>}}\"\n",
        "            )\n",
        "            resp = self.together_client.chat.completions.create(\n",
        "                model=self.RERANK_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0,\n",
        "            )\n",
        "            try:\n",
        "                score = int(json.loads(resp.choices[0].message.content)[\"score\"])\n",
        "            except (json.JSONDecodeError, KeyError, ValueError):\n",
        "                score = 1\n",
        "            scored.append((score, i, doc))\n",
        "\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        return [doc for _, _, doc in scored[:top_n]]\n",
        "\n",
        "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
        "        # Step 1: broad vector search\n",
        "        q_emb = self.embed([question])[0]\n",
        "        results = self.collection.query(query_embeddings=[q_emb], n_results=self.INITIAL_K)\n",
        "        candidates = results[\"documents\"][0]\n",
        "\n",
        "        # Step 2: LLM rerank and keep top FINAL_K\n",
        "        reranked = self._rerank_with_llm(question, candidates, self.FINAL_K)\n",
        "        return \"\\n\\n---\\n\\n\".join(reranked)\n",
        "\n",
        "    def generate(self, question: str, context: str) -> str:\n",
        "        system = (\n",
        "            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n",
        "            \"Use ONLY the provided context to answer. If the context doesn't contain enough \"\n",
        "            \"information, say so. Be precise and cite relevant articles when possible.\"\n",
        "        )\n",
        "        resp = self.together_client.chat.completions.create(\n",
        "            model=self.GEN_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"},\n",
        "            ],\n",
        "        )\n",
        "        return resp.choices[0].message.content"
      ],
      "execution_count": 27,
      "outputs": [],
      "id": "pe120xzjsts"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from collections import defaultdict\n",
        "from rank_bm25 import BM25Okapi\n",
        "import re as _re\n",
        "\n",
        "\n",
        "class HybridSearchRAG(RAGPipeline):\n",
        "    \"\"\"Combines BM25 keyword search with semantic vector search using RRF fusion.\"\"\"\n",
        "\n",
        "    name = \"hybrid_search_rag\"\n",
        "\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
        "    SEMANTIC_K = 10\n",
        "    BM25_K = 10\n",
        "    FINAL_K = 5\n",
        "    RRF_K = 60  # constant for Reciprocal Rank Fusion\n",
        "\n",
        "    def chunk(self, text: str) -> list[str]:\n",
        "        chunks, start = [], 0\n",
        "        while start < len(text):\n",
        "            chunks.append(text[start : start + self.CHUNK_SIZE])\n",
        "            start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n",
        "        return chunks\n",
        "\n",
        "    def build_index(self, text: str, batch_size: int = 100):\n",
        "        \"\"\"Build both vector index and BM25 index.\"\"\"\n",
        "        super().build_index(text, batch_size)\n",
        "\n",
        "        # Build BM25 index over the same chunks\n",
        "        tokenized = [self._tokenize(c) for c in self.chunks]\n",
        "        self.bm25 = BM25Okapi(tokenized)\n",
        "        print(f\"  BM25 index built over {len(self.chunks)} chunks\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _tokenize(text: str) -> list[str]:\n",
        "        return _re.findall(r\"\\w+\", text.lower())\n",
        "\n",
        "    def _bm25_search(self, query: str, k: int) -> list[tuple[int, float]]:\n",
        "        \"\"\"Return top-k (chunk_index, score) pairs from BM25.\"\"\"\n",
        "        tokens = self._tokenize(query)\n",
        "        scores = self.bm25.get_scores(tokens)\n",
        "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "        return [(i, scores[i]) for i in top_indices]\n",
        "\n",
        "    def _semantic_search(self, query: str, k: int) -> list[tuple[int, float]]:\n",
        "        \"\"\"Return top-k (chunk_index, rank_position) pairs from vector search.\"\"\"\n",
        "        q_emb = self.embed([query])[0]\n",
        "        results = self.collection.query(query_embeddings=[q_emb], n_results=k, include=[\"documents\"])\n",
        "        # ChromaDB returns ids like \"chunk_42\" — extract indices\n",
        "        indices = [int(cid.split(\"_\")[1]) for cid in results[\"ids\"][0]]\n",
        "        return [(idx, rank) for rank, idx in enumerate(indices)]\n",
        "\n",
        "    def _rrf_fuse(self, semantic_results: list[tuple[int, float]], bm25_results: list[tuple[int, float]]) -> list[int]:\n",
        "        \"\"\"Reciprocal Rank Fusion to merge two ranked lists.\"\"\"\n",
        "        scores = defaultdict(float)\n",
        "\n",
        "        for rank, (idx, _) in enumerate(semantic_results):\n",
        "            scores[idx] += 1.0 / (self.RRF_K + rank + 1)\n",
        "\n",
        "        for rank, (idx, _) in enumerate(bm25_results):\n",
        "            scores[idx] += 1.0 / (self.RRF_K + rank + 1)\n",
        "\n",
        "        fused = sorted(scores.keys(), key=lambda i: scores[i], reverse=True)\n",
        "        return fused[: self.FINAL_K]\n",
        "\n",
        "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
        "        semantic = self._semantic_search(question, self.SEMANTIC_K)\n",
        "        bm25 = self._bm25_search(question, self.BM25_K)\n",
        "        top_indices = self._rrf_fuse(semantic, bm25)\n",
        "        return \"\\n\\n---\\n\\n\".join(self.chunks[i] for i in top_indices)\n",
        "\n",
        "    def generate(self, question: str, context: str) -> str:\n",
        "        system = (\n",
        "            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n",
        "            \"Use ONLY the provided context to answer. If the context doesn't contain enough \"\n",
        "            \"information, say so. Be precise and cite relevant articles when possible.\"\n",
        "        )\n",
        "        resp = self.together_client.chat.completions.create(\n",
        "            model=self.GEN_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"},\n",
        "            ],\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "\n",
        "# ━━━ Switch pipeline here ━━━\n",
        "# Pipeline = NaiveRAG\n",
        "# Pipeline = RerankerRAG\n",
        "# Pipeline = HybridSearchRAG\n",
        "Pipeline = HybridSearchRAG"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2om8slw5xbx"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class HierarchicalRAG(RAGPipeline):\n",
        "    \"\"\"Two-level retrieval: search summaries first, then detailed chunks within matched sections.\"\"\"\n",
        "\n",
        "    name = \"hierarchical_rag\"\n",
        "\n",
        "    SECTION_SIZE = 5000       # large sections for summarisation\n",
        "    CHUNK_SIZE = 1000         # detailed chunks within each section\n",
        "    CHUNK_OVERLAP = 200\n",
        "    SUMMARY_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
        "    GEN_MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
        "    K_SUMMARIES = 3           # top sections to retrieve\n",
        "    K_CHUNKS = 5              # detailed chunks per selected section\n",
        "\n",
        "    def chunk(self, text: str) -> list[str]:\n",
        "        \"\"\"Not used directly — build_index handles both levels.\"\"\"\n",
        "        chunks, start = [], 0\n",
        "        while start < len(text):\n",
        "            chunks.append(text[start : start + self.CHUNK_SIZE])\n",
        "            start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n",
        "        return chunks\n",
        "\n",
        "    def _make_sections(self, text: str) -> list[str]:\n",
        "        \"\"\"Split text into large non-overlapping sections.\"\"\"\n",
        "        sections = []\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            sections.append(text[start : start + self.SECTION_SIZE])\n",
        "            start += self.SECTION_SIZE\n",
        "        return sections\n",
        "\n",
        "    def _summarise(self, section_text: str) -> str:\n",
        "        \"\"\"Ask the LLM for a concise summary of a section.\"\"\"\n",
        "        resp = self.together_client.chat.completions.create(\n",
        "            model=self.SUMMARY_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Summarise the following legal text in 2-3 sentences. Focus on the key articles, rights, or provisions mentioned.\"},\n",
        "                {\"role\": \"user\", \"content\": section_text[:4000]},\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "    def build_index(self, text: str, batch_size: int = 100):\n",
        "        \"\"\"Build two-level index: summary collection + detailed chunk collection.\"\"\"\n",
        "        sections = self._make_sections(text)\n",
        "        print(f\"[{self.name}] {len(sections)} sections of ~{self.SECTION_SIZE} chars\")\n",
        "\n",
        "        # ── Level 1: Summaries ──\n",
        "        self.summary_collection = self.chroma_client.get_or_create_collection(\n",
        "            name=\"hierarchical_summaries\", metadata={\"hnsw:space\": \"cosine\"},\n",
        "        )\n",
        "        print(\"  Generating summaries...\")\n",
        "        self.section_texts = sections\n",
        "        summaries = []\n",
        "        for i, sec in enumerate(sections):\n",
        "            summary = self._summarise(sec)\n",
        "            summaries.append(summary)\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"    Summarised {i + 1}/{len(sections)} sections\")\n",
        "\n",
        "        # Embed and store summaries\n",
        "        for i in range(0, len(summaries), batch_size):\n",
        "            batch = summaries[i : i + batch_size]\n",
        "            ids = [f\"summary_{j}\" for j in range(i, i + len(batch))]\n",
        "            embeddings = self.embed(batch)\n",
        "            self.summary_collection.upsert(ids=ids, documents=batch, embeddings=embeddings)\n",
        "        print(f\"  {len(summaries)} summaries indexed\")\n",
        "\n",
        "        # ── Level 2: Detailed chunks (with section_id metadata) ──\n",
        "        self.detailed_collection = self.chroma_client.get_or_create_collection(\n",
        "            name=\"hierarchical_details\", metadata={\"hnsw:space\": \"cosine\"},\n",
        "        )\n",
        "        all_chunks, all_ids, all_meta = [], [], []\n",
        "        chunk_counter = 0\n",
        "        for sec_idx, section in enumerate(sections):\n",
        "            start = 0\n",
        "            while start < len(section):\n",
        "                chunk_text = section[start : start + self.CHUNK_SIZE]\n",
        "                all_chunks.append(chunk_text)\n",
        "                all_ids.append(f\"detail_{chunk_counter}\")\n",
        "                all_meta.append({\"section_id\": sec_idx})\n",
        "                chunk_counter += 1\n",
        "                start += self.CHUNK_SIZE - self.CHUNK_OVERLAP\n",
        "\n",
        "        self.chunks = all_chunks  # keep for compatibility\n",
        "\n",
        "        for i in range(0, len(all_chunks), batch_size):\n",
        "            batch_docs = all_chunks[i : i + batch_size]\n",
        "            batch_ids = all_ids[i : i + batch_size]\n",
        "            batch_meta = all_meta[i : i + batch_size]\n",
        "            embeddings = self.embed(batch_docs)\n",
        "            self.detailed_collection.upsert(\n",
        "                ids=batch_ids, documents=batch_docs,\n",
        "                embeddings=embeddings, metadatas=batch_meta,\n",
        "            )\n",
        "            print(f\"  Embedded detail batch {i // batch_size + 1} ({len(batch_docs)} chunks)\")\n",
        "\n",
        "        print(f\"  {len(all_chunks)} detailed chunks indexed across {len(sections)} sections\")\n",
        "\n",
        "    def retrieve(self, question: str, n_results: int = 5) -> str:\n",
        "        \"\"\"Two-stage retrieval: summaries → detailed chunks within top sections.\"\"\"\n",
        "        q_emb = self.embed([question])[0]\n",
        "\n",
        "        # Stage 1: find top-K summary sections\n",
        "        summary_results = self.summary_collection.query(\n",
        "            query_embeddings=[q_emb], n_results=self.K_SUMMARIES,\n",
        "        )\n",
        "        top_section_ids = [\n",
        "            int(sid.split(\"_\")[1]) for sid in summary_results[\"ids\"][0]\n",
        "        ]\n",
        "\n",
        "        # Stage 2: search detailed chunks filtered to those sections\n",
        "        # ChromaDB supports $in filter on metadata\n",
        "        detail_results = self.detailed_collection.query(\n",
        "            query_embeddings=[q_emb],\n",
        "            n_results=self.K_CHUNKS,\n",
        "            where={\"section_id\": {\"$in\": top_section_ids}},\n",
        "            include=[\"documents\"],\n",
        "        )\n",
        "\n",
        "        return \"\\n\\n---\\n\\n\".join(detail_results[\"documents\"][0])\n",
        "\n",
        "    def generate(self, question: str, context: str) -> str:\n",
        "        system = (\n",
        "            \"You are a helpful assistant that answers questions about the Indian Constitution. \"\n",
        "            \"Use ONLY the provided context to answer. If the context doesn't contain enough \"\n",
        "            \"information, say so. Be precise and cite relevant articles when possible.\"\n",
        "        )\n",
        "        resp = self.together_client.chat.completions.create(\n",
        "            model=self.GEN_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"},\n",
        "            ],\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "Pipeline = HierarchicalRAG"
      ],
      "execution_count": 67,
      "outputs": [],
      "id": "z8pp72ckrif"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator"
      ],
      "id": "x3vvibuj6zo"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\"LLM-as-a-Judge scorer. Accepts RAG results and scores them against ground truth.\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = (\n",
        "        \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. \"\n",
        "        \"If the AI assistant's response is very close to the true response, assign a score of 1. \"\n",
        "        \"If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. \"\n",
        "        \"If the response is partially aligned with the true response, assign a score of 0.5. \"\n",
        "        'Respond with ONLY a JSON object: {\"score\": <0 | 0.5 | 1>}'\n",
        "    )\n",
        "\n",
        "    def __init__(self, judge_model: str = \"gpt-4o\"):\n",
        "        self.judge_model = judge_model\n",
        "        self.client = OpenAI()\n",
        "\n",
        "    def _score_one(self, question: str, ai_response: str, true_response: str) -> float:\n",
        "        prompt = (\n",
        "            f\"User Query: {question}\\n\"\n",
        "            f\"AI Response: {ai_response}\\n\"\n",
        "            f\"True Response: {true_response}\\n\"\n",
        "            f\"{self.SYSTEM_PROMPT}\"\n",
        "        )\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=self.judge_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0,\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "        )\n",
        "        return float(json.loads(resp.choices[0].message.content)[\"score\"])\n",
        "\n",
        "    def evaluate(self, results: list[dict], generation_time: float | None = None) -> list[dict]:\n",
        "        \"\"\"Score a list of RAG results and print aggregate metrics with timing.\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"  SCORING — Judge: {self.judge_model}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        eval_start = time.time()\n",
        "\n",
        "        for r in results:\n",
        "            r[\"score\"] = self._score_one(r[\"question\"], r[\"ai_response\"], r[\"answer\"])\n",
        "            label = {1: \"PASS\", 0.5: \"PARTIAL\", 0: \"FAIL\"}[r[\"score\"]]\n",
        "            print(f\"[{r['id']:>2}] {label:>7} ({r['score']})  {r['question'][:80]}\")\n",
        "\n",
        "        eval_time = time.time() - eval_start\n",
        "\n",
        "        # ── Aggregate ──\n",
        "        n = len(results)\n",
        "        total = sum(r[\"score\"] for r in results)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"  AGGREGATE SCORE — {n} questions\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"  Total: {total} / {n}  ({total / n:.1%})\")\n",
        "        print(f\"  PASS: {sum(1 for r in results if r['score'] == 1)}\")\n",
        "        print(f\"  PARTIAL: {sum(1 for r in results if r['score'] == 0.5)}\")\n",
        "        print(f\"  FAIL: {sum(1 for r in results if r['score'] == 0)}\")\n",
        "\n",
        "        # ── Timing ──\n",
        "        print(f\"\\n  TIMING\")\n",
        "        print(f\"  {'─'*50}\")\n",
        "        if generation_time is not None:\n",
        "            print(f\"  Generation:  {generation_time:.1f}s ({generation_time / n:.2f}s per question)\")\n",
        "        print(f\"  Evaluation:  {eval_time:.1f}s ({eval_time / n:.2f}s per question)\")\n",
        "        if generation_time is not None:\n",
        "            total_time = generation_time + eval_time\n",
        "            print(f\"  Total:       {total_time:.1f}s\")\n",
        "\n",
        "        # ── Per-category breakdown ──\n",
        "        categories = sorted(set(r[\"category\"] for r in results))\n",
        "        print(f\"\\n  PER-CATEGORY BREAKDOWN\")\n",
        "        print(f\"  {'─'*50}\")\n",
        "        for cat in categories:\n",
        "            cat_items = [r for r in results if r[\"category\"] == cat]\n",
        "            cat_total = sum(r[\"score\"] for r in cat_items)\n",
        "            cat_n = len(cat_items)\n",
        "            print(f\"  {cat} — {cat_total}/{cat_n} ({cat_total / cat_n:.1%})\")\n",
        "\n",
        "        return results"
      ],
      "execution_count": 4,
      "outputs": [],
      "id": "ihkmpek0r2q"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipeline = RerankerRAG()\n",
        "pipeline.build_index(all_text)\n",
        "\n",
        "gen_start = time.time()\n",
        "results = pipeline.generate_answers()\n",
        "gen_time = time.time() - gen_start\n",
        "\n",
        "evaluator = Evaluator(judge_model=\"gpt-4o\")\n",
        "eval_results = evaluator.evaluate(results, generation_time=gen_time)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[reranker_rag] 1097 chunks (avg 999 chars)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 4758 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 5092 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 5045 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 4737 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 4786 tokens\n",
            "  Embedded batch 1 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 4791 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 4679 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 5119 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 4848 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 4747 tokens\n",
            "  Embedded batch 2 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 4731 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 4899 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 4863 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 5121 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 4619 tokens\n",
            "  Embedded batch 3 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 4904 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 4628 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 5005 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 5000 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 5086 tokens\n",
            "  Embedded batch 4 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 4965 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 4582 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 4435 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 4790 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 4742 tokens\n",
            "  Embedded batch 5 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 4983 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 4867 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 4808 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 4661 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 4675 tokens\n",
            "  Embedded batch 6 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 5191 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 4618 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 4919 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 4833 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 5071 tokens\n",
            "  Embedded batch 7 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 5040 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 4827 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 4677 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 4754 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 5014 tokens\n",
            "  Embedded batch 8 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 6344 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 5393 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 5806 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 5079 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 4717 tokens\n",
            "  Embedded batch 9 (100 chunks)\n",
            "    [embed] 100 texts, chars: min=1000, max=1000, avg=1000\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 5081 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 4895 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 5136 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 4697 tokens\n",
            "    [embed] sub-batch 5: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 5: used 5151 tokens\n",
            "  Embedded batch 10 (100 chunks)\n",
            "    [embed] 97 texts, chars: min=750, max=1000, avg=997\n",
            "    [embed] sub-batch 1: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 1: used 6590 tokens\n",
            "    [embed] sub-batch 2: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 2: used 6496 tokens\n",
            "    [embed] sub-batch 3: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 3: used 4916 tokens\n",
            "    [embed] sub-batch 4: 20 items, 20000 total chars\n",
            "    [embed] sub-batch 4: used 6385 tokens\n",
            "    [embed] sub-batch 5: 17 items, 16750 total chars\n",
            "    [embed] sub-batch 5: used 6140 tokens\n",
            "  Embedded batch 11 (97 chunks)\n",
            "  Total in collection: 1097\n",
            "[reranker_rag] Running on 35 questions\n",
            "\n",
            "[1] Tier 1: Direct Retrieval\n",
            "Q: What is the minimum age required to be qualified for election as President of India?\n",
            "    [embed] 1 texts, chars: min=84, max=84, avg=84\n",
            "    [embed] sub-batch 1: 1 items, 84 total chars\n",
            "    [embed] sub-batch 1: used 17 tokens\n",
            "A: According to Article 58(1)(b) of the Constitution of India, a person shall be eligible for election as President unless he has completed the age of thirty-five years....\n",
            "------------------------------------------------------------\n",
            "\n",
            "[2] Tier 1: Direct Retrieval\n",
            "Q: Which article prohibits the employment of children in factories?\n",
            "    [embed] 1 texts, chars: min=64, max=64, avg=64\n",
            "    [embed] sub-batch 1: 1 items, 64 total chars\n",
            "    [embed] sub-batch 1: used 11 tokens\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m pipeline.build_index(all_text)\n\u001b[32m      4\u001b[39m gen_start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m gen_time = time.time() - gen_start\n\u001b[32m      8\u001b[39m evaluator = Evaluator(judge_model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mRAGPipeline.generate_answers\u001b[39m\u001b[34m(self, val_path)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m answer, context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m results.append({\n\u001b[32m    110\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: item[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    111\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m: item[\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: context,\n\u001b[32m    117\u001b[39m })\n\u001b[32m    118\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer[:\u001b[32m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mRAGPipeline.run\u001b[39m\u001b[34m(self, question)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     91\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"End-to-end: retrieve context then generate answer.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     answer = \u001b[38;5;28mself\u001b[39m.generate(question, context)\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m answer, context\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mRerankerRAG.retrieve\u001b[39m\u001b[34m(self, question, n_results)\u001b[39m\n\u001b[32m     48\u001b[39m candidates = results[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Step 2: LLM rerank and keep top FINAL_K\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m reranked = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rerank_with_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mFINAL_K\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(reranked)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mRerankerRAG._rerank_with_llm\u001b[39m\u001b[34m(self, query, documents, top_n)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(documents):\n\u001b[32m     24\u001b[39m     prompt = (\n\u001b[32m     25\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOn a scale of 1-10, rate how relevant the following document is to the query.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDocument:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc[:\u001b[32m800\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRespond with ONLY a JSON object: \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mscore\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: <integer 1-10>\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtogether_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mRERANK_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m         score = \u001b[38;5;28mint\u001b[39m(json.loads(resp.choices[\u001b[32m0\u001b[39m].message.content)[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/openai/_base_client.py:1297\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1288\u001b[39m     warnings.warn(\n\u001b[32m   1289\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1291\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1292\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1293\u001b[39m     )\n\u001b[32m   1294\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1295\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1296\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/openai/_base_client.py:1005\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1003\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1011\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalblogs/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.12/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.12/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "id": "d15c0f1b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": [],
      "id": "43e118a8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}